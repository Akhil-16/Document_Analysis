{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807b5d45",
   "metadata": {},
   "source": [
    "# All conetents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bfb2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b57b209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 0 content\n",
      "Introduction  \n",
      "The Waste Management Plan's goal is to explain the waste management concepts, processes, and \n",
      "management at the BML MUNJAL UNIVERSITY campus. This Plan was created by THE ACTIVISTS to \n",
      "ensure that waste is reduced, reused, and recycled as much as feasible.  \n",
      "The Plan contains information on the following topics:  \n",
      "1. The types and amounts of trash produced throughout the procedure.  \n",
      "2. Waste collection and disposal procedures.  \n",
      "3. Measures that will be taken to reduce the amount of trash generated as a result of the \n",
      "development.  \n",
      "\n",
      " \n",
      "page 1 content\n",
      "Abstract  \n",
      "Presently in India, about 960 million tonnes of solid waste is being generated annually as by -products \n",
      "of industrial, mining, municipal, agricultural and other processes. With rapid population growth and \n",
      "urbanization, this number will keep increasing at an unprecedented rate.  \n",
      " \n",
      "The substantial increase in solid waste generation results in the contamination of air, water, and land \n",
      "resources. Indian cities and towns are found littered with garbage, these practices create serious \n",
      "health, safety, and environmental consequences.  \n",
      " \n",
      "Poorly managed waste serves as a breeding ground for disease vectors and contributes to global \n",
      "climate change through methane generation.To safeguard the environment, efforts are being made for \n",
      "recycling different wastes and utilizing them in value -added a pplications. This report presents the \n",
      "status of the generation and utilization of solid wastes at BML Munjal University, their environmental \n",
      "implication is reported and discussed in detail.  \n",
      "\n",
      " \n",
      "page 2 content\n",
      "Problem Statement  \n",
      "A problem statement is a short, clear explanation of an issue or challenge that summarizes what you \n",
      "want to change. It helps you, team members, and other stakeholders to focus on the problem, why \n",
      "it's important, and who it impacts  \n",
      "\n",
      " \n",
      "page 3 content\n",
      "Contents  \n",
      "Introduction  ................................ ................................ ................................ ................................ ............  1 \n",
      "Abstract  ................................ ................................ ................................ ................................ ...................  2 \n",
      "Problem Statement  ................................ ................................ ................................ ................................ . 3 \n",
      "Literature Review  ................................ ................................ ................................ ................................ .... 5 \n",
      "Methodology  ................................ ................................ ................................ ................................ ...........  7 \n",
      "Refrences ................................ ................................ ................................ ................................ ...............  13 \n",
      "\n",
      " \n",
      "page 4 content\n",
      " \n",
      " \n",
      "Literature Review  \n",
      " \n",
      "The authors[1] proposed a method called LeicaGAN, consisting of a textual -visual co -embedding  \n",
      "network (TVE), a multiple priors aggregation network (MPA) and a cascaded attentive generator (CAG). The  \n",
      "text encoder was a pre -trained Bi -directional LSTM and the visual encoder was built upon the Inception -v3 \n",
      "model. The MPA network fused the sentence level embeddings. This acted as an input in the CAG, where an  \n",
      "attention block, two residual blocks, an upsampling block and a convolution layer make up the generator. Word  \n",
      "and Sentence -context features were produced. Two adversarial losses were employed: a visual realism  \n",
      "adversarial loss to ensure that the generators generate visually realistic images and a text -image pair -aware  \n",
      "adversarial loss to guarantee the semantic consistency between the input text and the generated image. For  \n",
      "effectiveness LeicaGAN was compared with AttnGAN. CUB and Oxford -102 datasets were used and evaluation  \n",
      "was done based on the Inception Score. LeicaGAN outperformed AttnGAN, on both the datasets.  \n",
      "\n",
      "The authors[2] proposed ControlGAN. For this, they introduced a word -level spatial and channel -wise \n",
      "attention -driven generator that could disentangle different visual attributes. Also, they proposed a word -level  \n",
      "discriminator. The backbone architecture they used was AttnGAN and the text encoder was a pre -trained bi  \n",
      "directional RNN. Conditioning Augmentation was applied. The generator exploited the attention mechanism via  \n",
      "incorporating a spatial attention module and the channel -wise attention module. The spatial attention module  \n",
      "dealt with words with individual spatial locations. The model was experimented on CUB and MS COCO  \n",
      "datasets. The model proposed was compared with AttnGAN and StackGAN++ and the performance metrics  \n",
      "were Inception Score, R -precision and L2 Error. ControlGAN gave the best results among the three for the CUB  \n",
      "dataset.(Inception Score = 4.58 ± 0.09, Top -1 Acc(%) = 69.33 ± 3.23, L2 error = 0.18). For the COCO dataset,  \n",
      "AttnGAN was the best in Inception Score and Top -1 Acc(%), but ControlGAN had the lowest L2 error, i.e.,  \n",
      "0.17.  \n",
      "\n",
      "The authors[4] proposed a StackGAN model. The model is built in two stages: Stage 1 GAN giving  \n",
      "Low Resolution images and Stage 2 GAN giving High resolution images. The model first processes the text  \n",
      "input and generates corresponding text embeddings to feed into the generative adversarial networks. The model  \n",
      "included a text encoder and decoder implemented with a word -level bidirectional recurrent neural network  \n",
      "(RNN) consisting of two long short -term memory (LSTM). The generator and discriminator receive a  \n",
      "conditioning variable. Dataset used was COCO. The pre -trained StackGAN model has decent performance on  \n",
      "generating images from a text input that is similar to its training set, although, when the input contains multiple  \n",
      "objects, StackGan fails to generate the correct number of instances with clear boundaries and spatial  \n",
      "relationships.  \n",
      "\n",
      "The authors[5] implemented DC -GAN conditioned on text features encoded by a hybrid character -level  \n",
      "convolutional recurrent neural network. In the generator, a text query was encoded. The description embedding  \n",
      "was compressed using a fully connected layer followed by LeakyReLU as the activation function. This was then  \n",
      "concatenated with the noise. The discriminator consisted of several layers of strides -2 convolution with spatial  \n",
      "batch normalization followed by LeakyReLU. The experiment was done on the CUB and Oxford -102 datasets.  \n",
      "The GAN baseline was compared with GAN -CLS with image -text matching discriminator, GAN -INT learned  \n",
      "with text manifold interpolation and GAN -INT-CLS which combined both.  \n",
      " \n",
      "This paper[6] described a transformer trained to autoregressive model the text and image tokens as a  \n",
      "single stream of data. A two -stage training procedure was used: Training a discrete Variational Autoencoder to  \n",
      "compress an 256 x 256 image into a 32 x 32 grid of image tokens and concatenating up to 256 BPE -encoded  \n",
      "text tokens with the 32 × 32 = 1024 image tokens and train an autoregressive transformer to model the joint  \n",
      "distribution over the text and image tokens. For a text -image pair, the lowercase caption is BPE -encoded using  \n",
      "at most 256 tokens with vocabulary size 16,384. The image is encoded using 32 × 32 = 1024 tokens with  \n",
      "page 5 content\n",
      "vocabulary size 8192. The image tokens are obtained using argmax sampling from the dVAE encoder logits. \n",
      "The text and image tokens are concatenated and modeled autoregressive as a single stream of data. The \n",
      "experiment was carried out of Conceptual Captions, an extension of MS COCO. The model is compared with \n",
      "AttnGAN,  DM-GAN and DF -GAN. The evaluation metrics were Inception Score and Fr échet Inception \n",
      "Distance. The  zero-shot model obtai ned an FID score on MS -COCO within 2 points of the best prior approach, \n",
      "despite having  \n",
      "never been trained on the captions. But, the model fares significantly worse on the CUB dataset, for which there  \n",
      "is a nearly 40 -point gap in FID between it and the leading prior approach, i.e., DM -GAN.  \n",
      "\n",
      "The authors[7] implemented a GAN -CLS including a generator and discriminator. The inputs were  \n",
      "batches of images and the matching text. Both the matching and mismatching text description are encoded,  \n",
      "noise is added. Three inputs are passed to the Discriminator: Correct Text with actual image, Incorrect Text with  \n",
      "actual image and Correct Text with fake image. These help in the better training of Discriminator. The dataset  \n",
      "used was the Oxford -102 flower dataset.  \n",
      "\n",
      "The authors[8] implemented a StackGAN to generate a stylised output image directly from the model.  \n",
      "The Stage 1 GAN generated low resolution images and a new conditioning is added to the Stage 2 GAN to  \n",
      "generate higher resolution images in a given style. The discriminator is trained on stylised 256 x 256 images.  \n",
      "The datasets used were COCO and CUB. No meaningful results were obtained from this method. The reason  \n",
      "given, too much time to reasonably generate enough stylized training data and to perform a hyper -parameter  \n",
      "search with enough iterations each time to find the best settings  \n",
      "\n",
      " \n",
      "page 6 content\n",
      " \n",
      " \n",
      "Methodology  \n",
      " \n",
      "Data was collected with the help of a device named “Mind Wave \n",
      "Mobile”  made by the NeuroSky company . It’s an EEG headset which \n",
      "is equipped with the biosensor technology that makes it easy for us to \n",
      "gather the EEG data from our brain.  A very useful feature of this device \n",
      "is that it gathers data and then do the basic pre -processing after which \n",
      "we can fetch the data in a .csv format.  \n",
      " \n",
      "Fig. 2: MindWave Mobile [2]  \n",
      "Process of data collection is as follows:  \n",
      "\n",
      "page 7 content\n",
      "1. Data is collected from a single human subject.  \n",
      "2. Subject is asked to sit in a room with minimal noise and \n",
      "disruption so that we can maintain data quality.  \n",
      "3. MindWave Mobile headset is mounted on the subject and correct \n",
      "placement of the headset is also necessary.  \n",
      "4. The ground of the headset is placed on the ear clip and the EEG \n",
      "sensor is placed on the end of the arm which is placed on the \n",
      "forehead.  \n",
      " \n",
      " Fig. 3: Subject during data collection  \n",
      "\n",
      "page 8 content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. The headset connects to the phone via Bluetooth, and some \n",
      "parameters must be set by the user in order for the data to be \n",
      "recorded.  \n",
      "6. At the time of data collection, the subjects consider one of four \n",
      "directions: forward, backward, left, or right.  \n",
      "7. The subject is asked to sit for an hour and think about the four \n",
      "mentioned directions one at a time  and data is stored in a .csv file.  \n",
      "8. The data is recorded for one minute at one -second intervals, \n",
      "yielding 60 rows of data in a single session.  \n",
      "9. This yields an average of 20 csv files, 5 in each direction, for a \n",
      "total of 1200 rows of data.  \n",
      " \n",
      "Once the process of data collection is completed then different Machine \n",
      "Learning and Neural Network Model are used for Classification  \n",
      " \n",
      "• LSTM  \n",
      " \n",
      "Fig. 4: Gated LSTM Cell  \n",
      "\n",
      "page 9 content\n",
      "The data is first processed,  basic algorithms for standardisation, \n",
      "outlier detection, etc. results in important features which are fed \n",
      "to the model. Then different layers are added to train the model \n",
      "and improve its accuracy.  \n",
      " \n",
      "• KNN  \n",
      " \n",
      "Fig. 5: KNN Diagram  \n",
      " \n",
      "The k -nearest neighbours algorithm, or KNN for short, is a \n",
      "supervised learning classifier that employs proximity to produce \n",
      "classifications or predictions about how a particular data point \n",
      "will be grouped. Although it can be applied to classification or \n",
      "regression issue s, it is commonly employed as a classification \n",
      "algorithm because it relies on the idea that comparable points can \n",
      "be discovered close to one another.  A class label is chosen for \n",
      "classification problems based on a majority vote, meaning that \n",
      "\n",
      "page 10 content\n",
      "the label that is most frequently represented around a particular \n",
      "data point is used.  \n",
      " \n",
      "• Random Forest  \n",
      " \n",
      "Fig. 6: Random Forest Diagram  \n",
      " \n",
      "A huge number of distinct decision trees work together as an \n",
      "ensemble in a random forest.  The term \"ensemble\" refers to a \n",
      "class of methods that combine several learning algorithms to \n",
      "provide predictions that are more accurate than those produced by \n",
      "any one of the individual learning algorithms used in the \n",
      "ensemble . Every single tree in the random forest spits out a class \n",
      "forecast, and the classification that receives the most votes \n",
      "becomes the prediction made by our model.  \n",
      " \n",
      "\n",
      "page 11 content\n",
      "Data Description  \n",
      " \n",
      "Over the course of four months, data was collected from a single \n",
      "subject in the directions forward, backward, left, and right.  The data set \n",
      "contained 15,459 rows in total. We are now working with 11 columns \n",
      "in this project after removing unnecessary columns.  \n",
      "This is how the data set appears : \n",
      " \n",
      "Fig. 7: Data Set  \n",
      "\n",
      "\n",
      "page 12 content\n",
      "Refrences  \n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    text = \"\"\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "       \n",
    "        for page_num in range(num_pages):\n",
    "            print(\"page\",page_num,\"content\" )\n",
    "            \n",
    "            page = reader.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "            \n",
    "            text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
    "            print(text)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "pdf_file = 'Introduction1.pdf'\n",
    "pdf_text = extract_text_from_pdf(pdf_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4346a37",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a77925f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Introduction', 1], ['Abstract', 2], ['Problem Statement', 3], ['Literature Review', 5], ['Methodology', 7], ['Refrences', 13]]\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_table_of_contents(pdf_file):\n",
    "    toc_pairs = []\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        toc_page = reader.pages[3]  \n",
    "        toc_text = toc_page.extract_text()\n",
    "        \n",
    "        toc_lines = toc_text.split('\\n')\n",
    "        for line in toc_lines:\n",
    "            parts = line.split('..') \n",
    "            if len(parts) >= 2: \n",
    "                heading = parts[0].strip()\n",
    "                \n",
    "              \n",
    "                page_num = parts[-1].split()[-1].strip()\n",
    "                \n",
    "               \n",
    "                if page_num.isdigit():\n",
    "                    toc_pairs.append([heading, int(page_num)])\n",
    "                else:\n",
    "                    print(f\"Skipping invalid page number: {page_num}\")\n",
    "                \n",
    "    return toc_pairs\n",
    "\n",
    "\n",
    "toc_pairs = extract_table_of_contents(pdf_file)\n",
    "print(toc_pairs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea4bfe",
   "metadata": {},
   "source": [
    "# Checking whether all topics are present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2611318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements are present in table of contents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list2 = ['introduction', 'methodology', 'abstract', 'Literature review']\n",
    "\n",
    "\n",
    "missing_elements = [element.lower() for element in list2 if element.lower() not in [item[0].lower() for item in toc_pairs]]\n",
    "\n",
    "\n",
    "if missing_elements:\n",
    "    print(\"These are the elemets that are missing in table of contents\")\n",
    "    for element in missing_elements:\n",
    "        print(element)\n",
    "else:\n",
    "    print(\"All elements are present in table of contents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56408779",
   "metadata": {},
   "source": [
    "# Find sematic similarity between intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ccd498",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def calculate_semantic_similarity(text1, text2):\n",
    "   \n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d3efc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Introduction  \n",
      "The Waste Management Plan's goal is to explain the waste management concepts, processes, and \n",
      "management at the BML MUNJAL UNIVERSITY campus. This Plan was created by THE ACTIVISTS to \n",
      "ensure that waste is reduced, reused, and recycled as much as feasible.  \n",
      "The Plan contains information on the following topics:  \n",
      "1. The types and amounts of trash produced throughout the procedure.  \n",
      "2. Waste collection and disposal procedures.  \n",
      "3. Measures that will be taken to reduce the amount of trash generated as a result of the \n",
      "development.  \n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "page_no_intro = None\n",
    "\n",
    "for x, y in toc_pairs:\n",
    "    x = x.lower() \n",
    "    if x == \"introduction\":\n",
    "        page_no_intro = y\n",
    "        break \n",
    "\n",
    "print(page_no_intro)\n",
    "def extract_text_from_page(pdf_file, page_number):\n",
    "    text = \"\"\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        if page_number < len(reader.pages):\n",
    "            page = reader.pages[page_number-1]\n",
    "            text = page.extract_text()\n",
    "            text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
    "#             text =text.replace(\"\\n\", \"\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "Intro_text = extract_text_from_page(pdf_file, page_no_intro)\n",
    "print(Intro_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac51ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c75f605",
   "metadata": {},
   "source": [
    "# Problem statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "680d8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Problem Statement  \n",
      "A problem statement is a short, clear explanation of an issue or challenge that summarizes what you \n",
      "want to change. It helps you, team members, and other stakeholders to focus on the problem, why \n",
      "it's important, and who it impacts  \n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x,y in toc_pairs:\n",
    "    x=x.lower()\n",
    "    if x=='problem statement':\n",
    "        \n",
    "        page_no_ps=y\n",
    "        break\n",
    "print(page_no_ps)   \n",
    "Ps_text = extract_text_from_page(pdf_file, page_no_ps)\n",
    "print(Ps_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b4d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df3d7520",
   "metadata": {},
   "source": [
    "## Literature Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "245daf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n \\nLiterature Review  \\n \\nThe authors[1] proposed a method called LeicaGAN, consisting of a textual -visual co -embedding  \\nnetwork (TVE), a multiple priors aggregation network (MPA) and a cascaded attentive generator (CAG). The  \\ntext encoder was a pre -trained Bi -directional LSTM and the visual encoder was built upon the Inception -v3 \\nmodel. The MPA network fused the sentence level embeddings. This acted as an input in the CAG, where an  \\nattention block, two residual blocks, an upsampling block and a convolution layer make up the generator. Word  \\nand Sentence -context features were produced. Two adversarial losses were employed: a visual realism  \\nadversarial loss to ensure that the generators generate visually realistic images and a text -image pair -aware  \\nadversarial loss to guarantee the semantic consistency between the input text and the generated image. For  \\neffectiveness LeicaGAN was compared with AttnGAN. CUB and Oxford -102 datasets were used and evaluation  \\nwas done based on the Inception Score. LeicaGAN outperformed AttnGAN, on both the datasets.  \\n\\nThe authors[2] proposed ControlGAN. For this, they introduced a word -level spatial and channel -wise \\nattention -driven generator that could disentangle different visual attributes. Also, they proposed a word -level  \\ndiscriminator. The backbone architecture they used was AttnGAN and the text encoder was a pre -trained bi  \\ndirectional RNN. Conditioning Augmentation was applied. The generator exploited the attention mechanism via  \\nincorporating a spatial attention module and the channel -wise attention module. The spatial attention module  \\ndealt with words with individual spatial locations. The model was experimented on CUB and MS COCO  \\ndatasets. The model proposed was compared with AttnGAN and StackGAN++ and the performance metrics  \\nwere Inception Score, R -precision and L2 Error. ControlGAN gave the best results among the three for the CUB  \\ndataset.(Inception Score = 4.58 ± 0.09, Top -1 Acc(%) = 69.33 ± 3.23, L2 error = 0.18). For the COCO dataset,  \\nAttnGAN was the best in Inception Score and Top -1 Acc(%), but ControlGAN had the lowest L2 error, i.e.,  \\n0.17.  \\n\\nThe authors[4] proposed a StackGAN model. The model is built in two stages: Stage 1 GAN giving  \\nLow Resolution images and Stage 2 GAN giving High resolution images. The model first processes the text  \\ninput and generates corresponding text embeddings to feed into the generative adversarial networks. The model  \\nincluded a text encoder and decoder implemented with a word -level bidirectional recurrent neural network  \\n(RNN) consisting of two long short -term memory (LSTM). The generator and discriminator receive a  \\nconditioning variable. Dataset used was COCO. The pre -trained StackGAN model has decent performance on  \\ngenerating images from a text input that is similar to its training set, although, when the input contains multiple  \\nobjects, StackGan fails to generate the correct number of instances with clear boundaries and spatial  \\nrelationships.  \\n\\nThe authors[5] implemented DC -GAN conditioned on text features encoded by a hybrid character -level  \\nconvolutional recurrent neural network. In the generator, a text query was encoded. The description embedding  \\nwas compressed using a fully connected layer followed by LeakyReLU as the activation function. This was then  \\nconcatenated with the noise. The discriminator consisted of several layers of strides -2 convolution with spatial  \\nbatch normalization followed by LeakyReLU. The experiment was done on the CUB and Oxford -102 datasets.  \\nThe GAN baseline was compared with GAN -CLS with image -text matching discriminator, GAN -INT learned  \\nwith text manifold interpolation and GAN -INT-CLS which combined both.  \\n \\nThis paper[6] described a transformer trained to autoregressive model the text and image tokens as a  \\nsingle stream of data. A two -stage training procedure was used: Training a discrete Variational Autoencoder to  \\ncompress an 256 x 256 image into a 32 x 32 grid of image tokens and concatenating up to 256 BPE -encoded  \\ntext tokens with the 32 × 32 = 1024 image tokens and train an autoregressive transformer to model the joint  \\ndistribution over the text and image tokens. For a text -image pair, the lowercase caption is BPE -encoded using  \\nat most 256 tokens with vocabulary size 16,384. The image is encoded using 32 × 32 = 1024 tokens with  vocabulary size 8192. The image tokens are obtained using argmax sampling from the dVAE encoder logits. \\nThe text and image tokens are concatenated and modeled autoregressive as a single stream of data. The \\nexperiment was carried out of Conceptual Captions, an extension of MS COCO. The model is compared with \\nAttnGAN,  DM-GAN and DF -GAN. The evaluation metrics were Inception Score and Fr échet Inception \\nDistance. The  zero-shot model obtai ned an FID score on MS -COCO within 2 points of the best prior approach, \\ndespite having  \\nnever been trained on the captions. But, the model fares significantly worse on the CUB dataset, for which there  \\nis a nearly 40 -point gap in FID between it and the leading prior approach, i.e., DM -GAN.  \\n\\nThe authors[7] implemented a GAN -CLS including a generator and discriminator. The inputs were  \\nbatches of images and the matching text. Both the matching and mismatching text description are encoded,  \\nnoise is added. Three inputs are passed to the Discriminator: Correct Text with actual image, Incorrect Text with  \\nactual image and Correct Text with fake image. These help in the better training of Discriminator. The dataset  \\nused was the Oxford -102 flower dataset.  \\n\\nThe authors[8] implemented a StackGAN to generate a stylised output image directly from the model.  \\nThe Stage 1 GAN generated low resolution images and a new conditioning is added to the Stage 2 GAN to  \\ngenerate higher resolution images in a given style. The discriminator is trained on stylised 256 x 256 images.  \\nThe datasets used were COCO and CUB. No meaningful results were obtained from this method. The reason  \\ngiven, too much time to reasonably generate enough stylized training data and to perform a hyper -parameter  \\nsearch with enough iterations each time to find the best settings  \\n\\n '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_review_pageno=None\n",
    "lit_review_end_pageno=None\n",
    "for index, pair in enumerate(toc_pairs):\n",
    "    x = pair[0].lower()\n",
    "    if x == 'literature review':\n",
    "      \n",
    "        lit_review_pageno=pair[1]    \n",
    "        if index + 1 < len(toc_pairs):\n",
    "            next_pair = toc_pairs[index + 1]\n",
    "            \n",
    "            lit_review_end_pageno=next_pair[1]\n",
    "            break\n",
    "Number_of_pages=lit_review_end_pageno-lit_review_pageno\n",
    "Number_of_pages\n",
    "lit_review_text=''\n",
    "for x in range(lit_review_pageno,lit_review_end_pageno):\n",
    "    lit_review_text=lit_review_text+extract_text_from_page(pdf_file,x)\n",
    "\n",
    "lit_review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d7ac634",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = lit_review_text.split(\"\\n\\n\")\n",
    "# Removing all the /n\n",
    "for index, paragraph in enumerate(paragraphs):\n",
    "    paragraphs[index] = paragraph.replace('\\n', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18fc5793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The authors[7] implemented a GAN -CLS including a generator and discriminator. The inputs were  batches of images and the matching text. Both the matching and mismatching text description are encoded,  noise is added. Three inputs are passed to the Discriminator: Correct Text with actual image, Incorrect Text with  actual image and Correct Text with fake image. These help in the better training of Discriminator. The dataset  used was the Oxford -102 flower dataset.  '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b75b7",
   "metadata": {},
   "source": [
    "### Entity Recognition for literature Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1856f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: ControlGAN, Label: ORGANIZATION\n",
      "Entity: AttnGAN, Label: ORGANIZATION\n",
      "Entity: RNN, Label: ORGANIZATION\n",
      "Entity: Augmentation, Label: PERSON\n",
      "Entity: CUB, Label: ORGANIZATION\n",
      "Entity: AttnGAN, Label: ORGANIZATION\n",
      "Entity: Inception Score, Label: PERSON\n",
      "Entity: R, Label: ORGANIZATION\n",
      "Entity: L2 Error, Label: PERSON\n",
      "Entity: ControlGAN, Label: ORGANIZATION\n",
      "Entity: CUB, Label: ORGANIZATION\n",
      "Entity: Inception Score, Label: ORGANIZATION\n",
      "Entity: Top -1 Acc, Label: PERSON\n",
      "Entity: L2, Label: GPE\n",
      "Entity: COCO, Label: ORGANIZATION\n",
      "Entity: AttnGAN, Label: ORGANIZATION\n",
      "Entity: Inception Score, Label: ORGANIZATION\n",
      "Entity: Top -1 Acc, Label: PERSON\n",
      "Entity: ControlGAN, Label: ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "def named_entity_recognition(text):\n",
    "  \n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "  \n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "  \n",
    "    chunked = ne_chunk(pos_tags)\n",
    "    \n",
    "\n",
    "    entities = []\n",
    "    for subtree in chunked:\n",
    "        if isinstance(subtree, nltk.Tree):\n",
    "            entity = \" \".join([word for word, tag in subtree.leaves()])\n",
    "            label = subtree.label()\n",
    "            entities.append((entity, label))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "entities = named_entity_recognition(paragraphs[1])\n",
    "\n",
    "for entity, label in entities:\n",
    "    print(f\"Entity: {entity}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2650507",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f0491d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.046*\".\" + 0.027*\",\" + 0.024*\"tokens\" + 0.024*\"image\" + 0.022*\"text\" + '\n",
      "  '0.017*\"32\" + 0.015*\"model\" + 0.012*\"GAN\" + 0.012*\"256\" + 0.012*\"using\"')]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def topic_modeling(text, num_topics=1):\n",
    "  \n",
    "    tokenized_text = word_tokenize(text)\n",
    "    \n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_text = [word for word in tokenized_text if word.lower() not in stop_words]\n",
    "    \n",
    "  \n",
    "    dictionary = corpora.Dictionary([filtered_text])\n",
    "    \n",
    "\n",
    "    bow_corpus = [dictionary.doc2bow(filtered_text)]\n",
    "    \n",
    " \n",
    "    lda_model = gensim.models.LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    \n",
    "\n",
    "    pprint(lda_model.print_topics())\n",
    "    \n",
    "    return lda_model\n",
    "\n",
    "\n",
    "\n",
    "lda_model = topic_modeling(paragraphs[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f40ebf",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bc01bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91701\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91701\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91701\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['attention', 'spatial', 'word', 'controlgan', 'error']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    " \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "def extract_keywords(text, num_keywords=5):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "\n",
    "  \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([preprocessed_text])\n",
    "\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "   \n",
    "    tfidf_scores = tfidf_matrix.toarray().flatten()\n",
    " \n",
    "\n",
    "   \n",
    "    top_indices = tfidf_scores.argsort()[-num_keywords:][::-1]\n",
    "    \n",
    "\n",
    "  \n",
    "    top_keywords = [feature_names[i] for i in top_indices]\n",
    "\n",
    "    return top_keywords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract keywords\n",
    "keywords = extract_keywords(paragraphs[1])\n",
    "print(\"Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e87b42",
   "metadata": {},
   "source": [
    "### Using semantic  similarity to verify the LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea076c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d7c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfbd3896",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99791c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Methodology   Data was collected with the help of a device named “Mind Wave Mobile”  made by the NeuroSky company . It’s an EEG headset which is equipped with the biosensor technology that makes it easy for us to gather the EEG data from our brain.  A very useful feature of this device is that it gathers data and then do the basic pre -processing after which we can fetch the data in a .csv format.   Fig. 2: MindWave Mobile [2]  Process of data collection is as follows:  1. Data is collected from a single human subject.  2. Subject is asked to sit in a room with minimal noise and disruption so that we can maintain data quality.  3. MindWave Mobile headset is mounted on the subject and correct placement of the headset is also necessary.  4. The ground of the headset is placed on the ear clip and the EEG sensor is placed on the end of the arm which is placed on the forehead.    Fig. 3: Subject during data collection  5. The headset connects to the phone via Bluetooth, and some parameters must be set by the user in order for the data to be recorded.  6. At the time of data collection, the subjects consider one of four directions: forward, backward, left, or right.  7. The subject is asked to sit for an hour and think about the four mentioned directions one at a time  and data is stored in a .csv file.  8. The data is recorded for one minute at one -second intervals, yielding 60 rows of data in a single session.  9. This yields an average of 20 csv files, 5 in each direction, for a total of 1200 rows of data.   Once the process of data collection is completed then different Machine Learning and Neural Network Model are used for Classification   • LSTM   Fig. 4: Gated LSTM Cell  The data is first processed,  basic algorithms for standardisation, outlier detection, etc. results in important features which are fed to the model. Then different layers are added to train the model and improve its accuracy.   • KNN   Fig. 5: KNN Diagram   The k -nearest neighbours algorithm, or KNN for short, is a supervised learning classifier that employs proximity to produce classifications or predictions about how a particular data point will be grouped. Although it can be applied to classification or regression issue s, it is commonly employed as a classification algorithm because it relies on the idea that comparable points can be discovered close to one another.  A class label is chosen for classification problems based on a majority vote, meaning that the label that is most frequently represented around a particular data point is used.   • Random Forest   Fig. 6: Random Forest Diagram   A huge number of distinct decision trees work together as an ensemble in a random forest.  The term \"ensemble\" refers to a class of methods that combine several learning algorithms to provide predictions that are more accurate than those produced by any one of the individual learning algorithms used in the ensemble . Every single tree in the random forest spits out a class forecast, and the classification that receives the most votes becomes the prediction made by our model.   Data Description   Over the course of four months, data was collected from a single subject in the directions forward, backward, left, and right.  The data set contained 15,459 rows in total. We are now working with 11 columns in this project after removing unnecessary columns.  This is how the data set appears :  Fig. 7: Data Set  '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Met_review_pageno=None\n",
    "Met_review_end_pageno=None\n",
    "for index, pair in enumerate(toc_pairs):\n",
    "    x = pair[0].lower()\n",
    "    if x == 'methodology':\n",
    "      \n",
    "        Met_review_pageno=pair[1]    \n",
    "        if index + 1 < len(toc_pairs):\n",
    "            next_pair = toc_pairs[index + 1]\n",
    "            \n",
    "            Met_review_end_pageno=next_pair[1]\n",
    "            break\n",
    "\n",
    "Met_review_text=''\n",
    "for x in range(Met_review_pageno,Met_review_end_pageno):\n",
    "    Met_review_text=Met_review_text+extract_text_from_page(pdf_file,x)\n",
    "\n",
    "Met_review_text=Met_review_text.replace(\"\\n\",\"\")\n",
    "Met_review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9778b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22e16b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extractive Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df807702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Data Description   Over the course of four months, data was collected from a single subject in the directions forward, backward, left, and right. At the time of data collection, the subjects consider one of four directions: forward, backward, left, or right. 4: Gated LSTM Cell  The data is first processed,  basic algorithms for standardisation, outlier detection, etc.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91701\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91701\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    words = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [[word for word in sentence if word not in stop_words] for sentence in words]\n",
    "\n",
    "    return sentences, filtered_words\n",
    "\n",
    "def calculate_word_frequencies(words):\n",
    "    # Flatten the list of words\n",
    "    flattened_words = [word for sentence in words for word in sentence]\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_freq = FreqDist(flattened_words)\n",
    "\n",
    "    return word_freq\n",
    "\n",
    "def calculate_sentence_scores(sentences, word_freq):\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        score = 0\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_freq:\n",
    "                score += word_freq[word]\n",
    "        sentence_scores[i] = score\n",
    "    return sentence_scores\n",
    "\n",
    "def generate_summary(text, num_sentences=3):\n",
    "    # Preprocess the text\n",
    "    sentences, words = preprocess_text(text)\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_freq = calculate_word_frequencies(words)\n",
    "\n",
    "    # Calculate sentence scores\n",
    "    sentence_scores = calculate_sentence_scores(sentences, word_freq)\n",
    "\n",
    "    # Get top scoring sentences\n",
    "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
    "\n",
    "    # Reconstruct the summary\n",
    "    summary = ' '.join([sentences[i] for i in top_sentences])\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example text\n",
    "\n",
    "\n",
    "# Generate summary\n",
    "summary = generate_summary(Met_review_text)\n",
    "print(\"Summary:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ac93eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 0 content\n",
      "page 1 content\n",
      "page 2 content\n",
      "page 3 content\n",
      "page 4 content\n",
      "page 5 content\n",
      "page 6 content\n",
      "page 7 content\n",
      "page 8 content\n",
      "page 9 content\n",
      "page 10 content\n",
      "page 11 content\n",
      "page 12 content\n",
      "['Introduction  ', 'Abstract  ', 'Problem Statement  ', 'Contents  ', ' ', 'vocabulary size 8192. The image tokens are obtained using argmax sampling from the dVAE encoder logits. ', ' ', '1. Data is collected from a single human subject.  ', '5. The headset connects to the phone via Bluetooth, and some ', 'The data is first processed,  basic algorithms for standardisation, ', 'the label that is most frequently represented around a particular ', 'Data Description  ', 'Refrences  ']\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "def extract_top_line_from_pdf(pdf_file):\n",
    "    top_lines = []\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "       \n",
    "        for page_num in range(num_pages):\n",
    "            print(\"page\", page_num, \"content\" )\n",
    "            \n",
    "            page = reader.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "            \n",
    "            # Split the text into lines and take the first line\n",
    "            lines = text.split('\\n')\n",
    "            if lines:\n",
    "                top_lines.append(lines[0])\n",
    "            else:\n",
    "                top_lines.append(\"\")  # Append an empty string if no text is extracted\n",
    "            \n",
    "    return top_lines\n",
    "\n",
    "# Example usage\n",
    "pdf_file = 'Introduction1.pdf'  # Replace with the path to your PDF file\n",
    "top_lines = extract_top_line_from_pdf(pdf_file)\n",
    "print(top_lines)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
