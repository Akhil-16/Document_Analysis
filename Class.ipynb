{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ea4a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import PyPDF2\n",
    "import textstat\n",
    "from language_tool_python import LanguageTool\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dabf0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:997)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:997)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:997)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bcab278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import textstat\n",
    "from language_tool_python import LanguageTool\n",
    "\n",
    "class Scoremaster:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.toc_pairs = []\n",
    "        self.text = \"\"\n",
    "        self.intro_text = \"\"\n",
    "        self.lit_text = \"\"\n",
    "        self.ps_text = \"\"\n",
    "        self.abs_text=\"\"\n",
    "        self.lit_paras = []\n",
    "        self.all_contents=[]\n",
    "        self.con_text=\"\"\n",
    "        \n",
    "\n",
    "    def extract_table_of_contents(self):\n",
    "        with open(self.file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in range(min(6, len(reader.pages))):  \n",
    "                toc_page = reader.pages[page_num]\n",
    "                toc_text = toc_page.extract_text()\n",
    "                toc_lines = toc_text.split('\\n')\n",
    "                for line in toc_lines:\n",
    "                    parts = line.split('........') \n",
    "                    if len(parts) >= 2: \n",
    "                        heading = parts[0].strip()\n",
    "                        page_num = parts[-1].split()[-1].strip()\n",
    "                        if page_num.isdigit():\n",
    "                            self.toc_pairs.append([heading, int(page_num)])\n",
    "                        else:\n",
    "                            print(f\"Skipping invalid page number: {page_num}\")\n",
    "    def Checking_Chronology(self, list2):\n",
    "        All_elements_present = False\n",
    "        missing_elements = [element.lower() for element in list2 if element.lower().replace(\" \", \"\") not in [item[0].lower().replace(\" \", \"\") for item in self.toc_pairs]]\n",
    "        if missing_elements:\n",
    "            print(\"These are the elements that are missing in the table of contents:\")\n",
    "            for element in missing_elements:\n",
    "                print(element)\n",
    "        else:\n",
    "            All_elements_present = True\n",
    "            print(\"All elements are present in the table of contents.\")\n",
    "        if All_elements_present:\n",
    "            pointer = 0\n",
    "            for x, y in self.toc_pairs:\n",
    "                if x.lower().replace(\" \", \"\") == list2[pointer].lower().replace(\" \", \"\"):\n",
    "                    pointer += 1\n",
    "                else:\n",
    "                    print(\"Table of contents is not in proper order.\")\n",
    "                    print(x.lower().replace(\" \", \"\"),list2[pointer].lower().replace(\" \", \"\"))\n",
    "                    break\n",
    "\n",
    "    def extract_text_from_page(self, page_number):  \n",
    "        with open(self.file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            if page_number < len(reader.pages):\n",
    "                page = reader.pages[page_number-1]\n",
    "                text = page.extract_text()\n",
    "                text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
    "                return text\n",
    "\n",
    "    def extract_intro_text(self):\n",
    "        intro_start_page = None\n",
    "        intro_end_page = None\n",
    "        for index, pair in enumerate(self.toc_pairs):\n",
    "            if pair[0].lower() == 'introduction':\n",
    "                intro_start_page = pair[1]    \n",
    "                if index + 1 < len(self.toc_pairs):\n",
    "                    next_pair = self.toc_pairs[index + 1]\n",
    "                    intro_end_page = next_pair[1]\n",
    "                    break\n",
    "\n",
    "        if intro_start_page is not None:\n",
    "            self.intro_text = \"\"\n",
    "            for page_num in range(intro_start_page, intro_end_page):\n",
    "                self.intro_text += self.extract_text_from_page(page_num)\n",
    "            self.all_contents.append(self.intro_text)\n",
    "            return self.intro_text\n",
    "        else:\n",
    "            print(\"Introduction section not found in table of contents.\")\n",
    "\n",
    "    def extract_ps_text(self):\n",
    "        ps_start_page = None\n",
    "        ps_end_page = None\n",
    "        for index, pair in enumerate(self.toc_pairs):\n",
    "            if pair[0].lower() == 'problem statement':\n",
    "                ps_start_page = pair[1]    \n",
    "                if index + 1 < len(self.toc_pairs):\n",
    "                    next_pair = self.toc_pairs[index + 1]\n",
    "                    ps_end_page = next_pair[1]\n",
    "                    break\n",
    "\n",
    "        if ps_start_page is not None:\n",
    "            self.ps_text = \"\"\n",
    "            for page_num in range(ps_start_page, ps_end_page):\n",
    "                self.ps_text += self.extract_text_from_page(page_num)\n",
    "#             self.all_contents.append(self.ps_text)\n",
    "        else:\n",
    "            print(\"Problem Statement section not found in table of contents.\")\n",
    "    def extract_conclusion_text(self):\n",
    "        con_start_page = None\n",
    "        con_end_page = None\n",
    "        for index, pair in enumerate(self.toc_pairs):\n",
    "            if pair[0].lower() == 'conclusion':\n",
    "                con_start_page = pair[1]    \n",
    "                if index + 1 < len(self.toc_pairs):\n",
    "                    next_pair = self.toc_pairs[index + 1]\n",
    "                    con_end_page = next_pair[1]\n",
    "                    break\n",
    "\n",
    "        if con_start_page is not None:\n",
    "            self.con_text = \"\"\n",
    "            for page_num in range(con_start_page, con_end_page):\n",
    "                self.con_text += self.extract_text_from_page(page_num)\n",
    "#             self.all_contents.append(self.con_text)\n",
    "            return self.con_text\n",
    "        else:\n",
    "            print(\"Conclusion  section not found in table of contents.\")\n",
    "            \n",
    "         \n",
    "\n",
    "    def extract_lit_text(self):\n",
    "        lit_start_page = None\n",
    "        lit_end_page = None\n",
    "        for index, pair in enumerate(self.toc_pairs):\n",
    "            if pair[0].lower() == 'literature review':\n",
    "                lit_start_page = pair[1]    \n",
    "                if index + 1 < len(self.toc_pairs):\n",
    "                    next_pair = self.toc_pairs[index + 1]\n",
    "                    lit_end_page = next_pair[1]\n",
    "                    break\n",
    "\n",
    "        if lit_start_page is not None:\n",
    "            self.lit_text = \"\"\n",
    "            for page_num in range(lit_start_page, lit_end_page):\n",
    "                self.lit_text += self.extract_text_from_page(page_num)\n",
    "            self.lit_paras = self.lit_text.split(\"\\n\\n\")\n",
    "            for index, paragraph in enumerate(self.lit_paras):\n",
    "                self.lit_paras[index] = paragraph.replace('\\n', '')\n",
    "            self.lit_paras = [p for p in self.lit_paras if p.strip()]\n",
    "            return self.lit_paras\n",
    "            \n",
    "        else:\n",
    "            print(\"Literature Review section not found in table of contents.\")\n",
    "    \n",
    "    def extract_abs_text(self):\n",
    "        abs_start_page=None\n",
    "        abs_end_page=None\n",
    "        for index, pair in enumerate(self.toc_pairs):\n",
    "            if pair[0].lower() == 'abstract':\n",
    "                abs_start_page = pair[1]    \n",
    "                if index + 1 < len(self.toc_pairs):\n",
    "                    next_pair = self.toc_pairs[index + 1]\n",
    "                    abs_end_page = next_pair[1]\n",
    "                    break\n",
    "\n",
    "        if abs_start_page is not None:\n",
    "            self.abs_text = \"\"\n",
    "            for page_num in range(abs_start_page, abs_end_page):\n",
    "                self.abs_text += self.extract_text_from_page(page_num)\n",
    "#             self.all_contents.append(self.abs_text)\n",
    "            return self.abs_text\n",
    "            \n",
    "        else:\n",
    "            print(\" Abstract section not found in table of contents.\")\n",
    "        \n",
    "        \n",
    "    def analyze_text(self):\n",
    "        def calculate_flesch_reading_ease(text):\n",
    "            return textstat.flesch_reading_ease(text)\n",
    "\n",
    "        def evaluate_readability(score):\n",
    "            if score is None:\n",
    "                return \"Unknown\", 0\n",
    "            elif score >= 90:\n",
    "                return \"Very Easy\", 100\n",
    "            elif score >= 80:\n",
    "                return \"Easy\", 90\n",
    "            elif score >= 70:\n",
    "                return \"Fairly Easy\", 80\n",
    "            elif score >= 60:\n",
    "                return \"Standard\", 70\n",
    "            elif score >= 50:\n",
    "                return \"Fairly Difficult\", 60\n",
    "            elif score >= 30:\n",
    "                return \"Difficult\", 50\n",
    "            else:\n",
    "                return \"Very Confusing\", 40\n",
    "\n",
    "        def check_grammar(text):\n",
    "            tool = LanguageTool('en-US')\n",
    "            matches = tool.check(text)\n",
    "\n",
    "            \n",
    "            excluded_rule_ids = ['CONSECUTIVE_SPACES', 'MORFOLOGIK_RULE_EN_US','WHITESPACE_RULE']\n",
    "            matches = [match for match in matches if match.ruleId not in excluded_rule_ids]\n",
    "\n",
    "            return matches\n",
    "\n",
    "\n",
    "        def evaluate_grammar_quality(matches):\n",
    "            num_issues = len(matches)\n",
    "            if num_issues == 0:\n",
    "                return \"Good\", 100\n",
    "            elif num_issues < 5:\n",
    "                return \"Acceptable\", 80\n",
    "            elif num_issues < 10:\n",
    "                return \"Poor\", 60\n",
    "            else:\n",
    "                return \"Very Poor\", 40\n",
    "        average=0\n",
    "        for section_text in self.all_contents:\n",
    "            \n",
    "            section_text = section_text.replace(\"\\n\", \"\")\n",
    "            if section_text:\n",
    "                flesch_score = calculate_flesch_reading_ease(section_text)\n",
    "                readability, readability_score = evaluate_readability(flesch_score)\n",
    "                grammar_matches = check_grammar(section_text)\n",
    "                grammar_quality, grammar_score = evaluate_grammar_quality(grammar_matches)\n",
    "                average=average+(flesch_score+readability_score+grammar_score)/3\n",
    "                print(\"Section Analysis:\")\n",
    "                print(f\"Flesch Reading Ease score: {flesch_score}\")\n",
    "                print(f\"Readability: {readability} ({readability_score})\")\n",
    "                print(f\"Grammar quality: {grammar_quality} ({grammar_score})\")\n",
    "                print()\n",
    "        return average,grammar_quality,readability\n",
    "    def extract_keywords(self,text, num_keywords=5):\n",
    "            \n",
    "        def preprocess_text(text):\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "            return \" \".join(lemmatized_tokens)\n",
    "\n",
    "        preprocessed_text = preprocess_text(text)\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([preprocessed_text])\n",
    "\n",
    "        \n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "      \n",
    "        tfidf_scores = tfidf_matrix.toarray().flatten()\n",
    "\n",
    "      \n",
    "        top_indices = tfidf_scores.argsort()[-num_keywords:][::-1]\n",
    "\n",
    "        \n",
    "        top_keywords = [feature_names[i] for i in top_indices]\n",
    "\n",
    "        return top_keywords\n",
    "    def calculate_semantic_similarity(self,text1, text2):\n",
    "\n",
    "   \n",
    "        doc1 = nlp(text1)\n",
    "        doc2 = nlp(text2)\n",
    "\n",
    "        similarity = doc1.similarity(doc2)\n",
    "        return similarity*100\n",
    "    \n",
    "    def extract_numbers(self,text):\n",
    "        # Regular expression pattern to match numbers within square brackets\n",
    "        pattern = r'\\[(\\d+)\\]'\n",
    "\n",
    "        # Find all matches of the pattern in the text\n",
    "        matches = re.findall(pattern, text)\n",
    "\n",
    "        # Convert matched numbers from strings to integers and store them in a list\n",
    "        numbers = [int(match) for match in matches]\n",
    "\n",
    "        return numbers\n",
    "\n",
    "\n",
    "\n",
    "    def refrences(self):\n",
    "        page_no_ref=None\n",
    "        for x,y in self.toc_pairs:\n",
    "            x=x.lower()\n",
    "            if x=='references':\n",
    "                page_no_ref=y\n",
    "                break\n",
    "        return page_no_ref \n",
    "    def extract_paragraphs_with_numbers(self,pdf_file, page_number, numbers):\n",
    "        paragraphs = []\n",
    "\n",
    "        with open(pdf_file, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            page = reader.pages[page_number - 1]  # Adjusting for 0-based indexing\n",
    "\n",
    "            text = page.extract_text()\n",
    "\n",
    "            lines = text.split('\\n')\n",
    "\n",
    "            for line in lines:\n",
    "                # Extract the starting number from the line\n",
    "                match = re.match(r'^\\d+', line)\n",
    "                if match:\n",
    "                    starting_number = int(match.group())\n",
    "\n",
    "                    # Check if the starting number matches any number in the given list\n",
    "                    if starting_number in numbers:\n",
    "                        paragraphs.append(line)\n",
    "\n",
    "        return paragraphs\n",
    "\n",
    "\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39e86d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements are present in the table of contents.\n"
     ]
    }
   ],
   "source": [
    "list2 = ['Introduction', 'Abstract', 'Problem Statement','List of Figures', 'Literature Review', 'Methodology','Conclusion','References']\n",
    "\n",
    "lit_keywords=[]\n",
    "pdf_file = 'Introduction1.pdf'\n",
    "scoremaster = Scoremaster(pdf_file)\n",
    "scoremaster.extract_table_of_contents()\n",
    "scoremaster.Checking_Chronology(list2)\n",
    "intro_text=scoremaster.extract_intro_text()\n",
    "scoremaster.extract_ps_text()\n",
    "abstract_text=scoremaster.extract_abs_text()\n",
    "conclusion=scoremaster.extract_conclusion_text()\n",
    "list_of_literature_reviews=scoremaster.extract_lit_text()\n",
    "# grammer_score,grammar_quality,Readability=scoremaster.analyze_text()\n",
    "lit_text=\"\".join(list_of_literature_reviews)\n",
    "refrence_page_no=scoremaster.refrences()\n",
    "for x in range(len(scoremaster.lit_paras)):\n",
    "    lit_keywords.append(scoremaster.extract_keywords(scoremaster.lit_paras[x]))\n",
    "    \n",
    "abs_intro_similarity=scoremaster.calculate_semantic_similarity(abstract_text,intro_text)\n",
    "abs_con_sim=scoremaster.calculate_semantic_similarity(abstract_text,conclusion)\n",
    "intro_con=scoremaster.calculate_semantic_similarity(intro_text,conclusion)\n",
    "numbers=scoremaster.extract_numbers(lit_text)\n",
    "\n",
    "matching_ref_paragraphs = scoremaster.extract_paragraphs_with_numbers(pdf_file, refrence_page_no, numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edc92eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Nunez PL, Srinivasan R.  Electric fields of the brain: the ',\n",
       " '2. NeuroSky. 2015 MindWave. Retrieved from ',\n",
       " '4. Craik, A., He, Y., & Vidal, J. (2019). Deep learning for ',\n",
       " '5. https://www.hopkinsmedicine.org/health/treatment -tests-and-',\n",
       " '6. https://imotions.com/blog/what -is-eeg/ ',\n",
       " '7. https://www.ibm.com/topics/knn  ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_ref_paragraphs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
